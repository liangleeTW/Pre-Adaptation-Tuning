% ========================================
% OUTLINE: Models Subsection (Methods)
% ========================================
%
% What Reviewers Expect:
% 1. Conceptual Introduction (why model proprioceptive tuning effects?)
% 2. Core Model Framework (Kalman filter formulation)
% 3. Model Variants (M1-null → M1-add → M1-exp → M1-tanh → M2-obs → M2-dual)
%    - Each with: formulation, rationale, prediction
% 4. Key Innovation (separating R_state from R_obs)
% 5. Model Comparison Strategy (WAIC, LOO, convergence)
% 6. Implementation Details (priors, inference)
% 7. Models Excluded (transparency about non-converged models)
%
% Structure Flow:
% - Start with motivation (why these models?)
% - Build from simple to complex
% - Emphasize M2-dual as key contribution
% - End with comparison strategy
% ========================================

\section{Method}
\subsection{Computational Models}

\subsubsection{Overview}

To test whether proprioceptive tuning modulates uncertainty during visuomotor learning, we compared a family of state-space models differing in how changes in proprioceptive precision ($\Delta\log\pi$) influence sensory noise parameters. Our modeling approach follows a progression from simpler single-noise models (M1-null, M1-add, M1-exp, M1-tanh) that treat measurement noise as a unified parameter, to dual-noise models (M2-obs, M2-dual) that explicitly separate state uncertainty from observation noise—a distinction critical for capturing both learning dynamics and trial-to-trial variability.

\subsubsection{Core State-Space Framework}

All models assume subjects learn an internal estimate $x_t$ of the visual perturbation magnitude via a Kalman filter with state transition and observation equations:
\begin{align}
  x_{t+1} &= A\,x_t + w_t, && w_t \sim \mathcal{N}(0,Q) \label{eq:state}\\
  y_t     &= -x_t + m + b + \varepsilon_t, && \varepsilon_t \sim \mathcal{N}(0,R) \label{eq:obs}
\end{align}
where $x_t$ is the internal perturbation estimate (compensation magnitude), $y_t$ is the observed reach error, $m$ is the true perturbation magnitude ($\approx -12°$), $b$ is a plateau offset, $A=1$ (perfect retention), and $Q$ is process noise (set to $10^{-4}$ for near-deterministic state updates). The observation model uses $h=-1$ because errors decrease as compensation increases.

The Kalman gain, which determines learning speed, is given by:
\begin{equation}
  K_t = \frac{-P_t}{P_t + R}
\end{equation}
where $P_t$ is the state covariance. Larger $R$ (higher measurement noise) → smaller gain → slower learning from visual errors. This provides the mechanistic link between proprioceptive tuning and adaptation speed.

\subsubsection{Model Variants: How Does Proprioceptive Tuning Modulate Uncertainty?}

We tested six models representing different hypotheses about how proprioceptive tuning affects learning. We first describe four \emph{single-noise models} (M1-null through M1-tanh) that treat measurement noise as a unified parameter, followed by two \emph{dual-noise models} (M2-obs and M2-dual) that separate state uncertainty from observation noise.

\paragraph{Single-Noise Models}

\textit{M1-null (Baseline).} This null model assumes proprioceptive tuning has no effect on visuomotor learning:
\begin{equation}
  R = R_\text{post1} \label{eq:m1null}
\end{equation}
where measurement noise remains fixed at the post-adaptation baseline proprioceptive uncertainty. Under this model, we predict no relationship between $\Delta\log\pi$ and learning speed, serving as the reference against which to evaluate whether proprioceptive tuning matters at all.

\textit{M1-add (Additive Modulation).} The simplest extension introduces a linear relationship between proprioceptive tuning and measurement noise:
\begin{equation}
  R = R_\text{post1} + \beta\,\Delta\log\pi \label{eq:m1add}
\end{equation}
If $\beta > 0$, sharper proprioception increases $R$, leading to slower learning from visual errors. This additive form directly quantifies how much measurement noise changes per unit change in proprioceptive precision. However, this specification can produce negative $R$ if $\beta\,\Delta\log\pi < -R_\text{post1}$, requiring parameter constraints during fitting.

\textit{M1-exp (Exponential Modulation).} To ensure positivity and enable proportional scaling, we adopt an exponential (log-link) formulation:
\begin{equation}
  R = R_\text{post1}\,\exp(\beta\,\Delta\log\pi) \label{eq:m1exp}
\end{equation}
Here $\beta$ represents the elasticity of measurement noise with respect to proprioceptive tuning—the proportional change in $R$ per unit change in $\Delta\log\pi$. This model predicts the same directional effect as M1-add (slower learning when $\beta > 0$) while guaranteeing $R > 0$ and naturally accommodating multiplicative effects. As we show in Results, M1-exp was the best-performing single-noise model.

\textit{M1-tanh (Bounded Modulation).} To impose saturation limits and prevent extreme noise values, we test a bounded formulation:
\begin{equation}
  R = R_\text{post1}\,(1 - \lambda\,\tanh(\Delta\pi)) \label{eq:m1tanh}
\end{equation}
This model predicts diminishing returns at extreme tuning values, with $\lambda$ controlling the maximum proportional change in $R$. However, when observed $\Delta\pi$ values are small (as in our data), the tanh saturation can over-constrain effects, potentially masking linear relationships that M1-exp would capture.

\paragraph{Dual-Noise Models}

Standard Kalman filter models (M1-null through M1-tanh) conflate two conceptually distinct noise sources: \emph{state uncertainty} ($R_\text{state}$), which reflects uncertainty about the internal model estimate and determines the Kalman gain (learning rate), and \emph{observation noise} ($R_\text{obs}$), which captures trial-to-trial variability in motor execution and measurement that does not affect learning but adds scatter to observed errors. Treating these as a single parameter $R$ forces the model to choose between fitting early learning dynamics—when $P_t$ is large and gain is high—or late-trial oscillations—when $P_t$ converges but observations remain noisy. This tradeoff is the root cause of the negative $R^2$ values observed in single-noise models (see Results). To address this limitation, we developed two dual-noise models that explicitly separate these sources.

\textit{M2-obs (Observation Noise Only).} This model tests whether proprioceptive tuning affects execution variability without altering learning speed:
\begin{align}
  R_\text{state} &= R_\text{post1} \label{eq:m2obs_state}\\
  R_\text{obs}   &= R_{\text{obs},0}\,\exp(\beta_\text{obs}\,\Delta\log\pi) \label{eq:m2obs_obs}
\end{align}
Under M2-obs, the Kalman gain remains fixed (controlled by $R_\text{state} = R_\text{post1}$), while proprioceptive tuning modulates only the scatter in observed errors through $\beta_\text{obs}$. If $\beta_\text{obs} > 0$, sharper proprioception would increase late-trial variability without changing the rate of early learning. Conversely, if $\beta_\text{obs} < 0$, proprioceptive tuning would tighten motor execution. This model isolates the execution-variability hypothesis while holding learning dynamics constant.

\textit{M2-dual (Full Dual Modulation).} Our primary model allows proprioceptive tuning to independently affect both state uncertainty and observation noise:
\begin{align}
  R_\text{state} &= R_\text{post1}\,\exp(\beta_\text{state}\,\Delta\log\pi) \label{eq:m2dual_state}\\
  R_\text{obs}   &= R_{\text{obs},0}\,\exp(\beta_\text{obs}\,\Delta\log\pi) \label{eq:m2dual_obs}
\end{align}
M2-dual is the most general formulation, permitting $\Delta\log\pi$ to modulate learning speed via $\beta_\text{state}$ (which affects the Kalman gain) and execution variability via $\beta_\text{obs}$ (which affects late-trial scatter). If $\beta_\text{state} > 0$, we predict slower learning with higher proprioceptive precision—testable via state trajectory analysis (see Results). If $\beta_\text{obs} \neq 0$, proprioceptive tuning modulates late-trial variability. Critically, M2-dual can fit both early learning curves and late-trial oscillations simultaneously, yielding positive $R^2$ values where single-noise models fail.

In implementation, the Kalman filter computes two innovation variances:
\begin{align}
  s_\text{kalman} &= P_t + R_\text{state} && \text{(for gain computation)} \label{eq:s_kalman}\\
  s_\text{total}  &= P_t + R_\text{state} + R_\text{obs} && \text{(for likelihood)} \label{eq:s_total}
\end{align}
The Kalman gain $K_t = -P_t / s_\text{kalman}$ determines learning, using only state uncertainty, while the log-likelihood $\mathcal{L} \propto -\log s_\text{total} - v_t^2/s_\text{total}$ accounts for total observed variability. This separation is the key innovation enabling M2-dual to capture both learning dynamics and trial-to-trial noise.

\subsubsection{Models Excluded}

Two additional models were tested but excluded from final analysis due to convergence failures:
\begin{itemize}
  \item \textbf{M-Q (Process Noise Modulation):} Modulates $Q$ instead of $R$. Failed to converge ($\hat{R} > 1.5$, ESS $< 10$) due to identifiability issues when both $P_t$ and $Q$ vary.
  \item \textbf{M-hybrid (Joint R+Q):} Simultaneously modulates both $R$ and $Q$ with a single noise term. Suffered similar non-convergence and parameter trade-offs.
\end{itemize}
Details are provided in Supplementary Materials.

\subsubsection{Model Comparison}

Models were compared using WAIC (Widely Applicable Information Criterion), which balances fit and complexity with lower values indicating better performance, and LOO (Leave-One-Out cross-validation) for out-of-sample predictive accuracy. Convergence was assessed via $\hat{R} < 1.01$ and ESS$_\text{bulk} > 400$. Posterior predictive checks included one-step-ahead $R^2$ and residual analysis to evaluate model quality beyond information criteria.

All models were fit using Hamiltonian Monte Carlo (NUTS sampler) in NumPyro with 4 chains, 2000 warmup iterations, and 2000 sampling iterations per chain. Priors were weakly informative: $\beta \sim \mathcal{N}(0, 1)$, $R_{\text{obs},0} \sim \text{HalfNormal}(2)$, $b \sim \mathcal{N}(0, 30)$ (group-specific plateaus).


% ========================================
% Results Section
% ========================================

\section{Results}

\subsection{Dual-Noise Separation Dramatically Improves Model Fit}

We compared six models differing in how proprioceptive tuning ($\Delta\log\pi$) modulates uncertainty during visuomotor adaptation. Model comparison via WAIC revealed that M2-dual, which separately models state uncertainty and observation noise, substantially outperformed all single-noise models (Table~\ref{tab:model_comparison}). M2-dual achieved a WAIC of 33,469, representing an improvement of $\Delta$WAIC = $-$10,956 relative to M1-exp (WAIC = 44,425), the best-performing single-noise model. This difference far exceeds conventional thresholds for decisive model selection ($\Delta$WAIC $>$ 10), indicating that separating state uncertainty from observation noise is critical for capturing visuomotor learning dynamics.

The superiority of M2-dual was further confirmed by posterior predictive checks (Figure~\ref{fig:ppc}). While M1-exp exhibited negative one-step-ahead prediction accuracy ($R^2 = -1.05$), indicating predictions worse than simply using the mean error, M2-dual achieved positive predictive accuracy ($R^2 = +0.027$). This dramatic shift from negative to positive $R^2$ reflects M2-dual's ability to simultaneously fit both early learning dynamics and late-trial oscillations—a tradeoff that single-noise models cannot resolve. Root-mean-squared error (RMSE) similarly improved from 4.12° for M1-exp to 2.76° for M2-dual, representing a 33\% reduction in prediction error. Systematic bias in late-trial predictions, which reached $-$3.09° for M1-exp (overpredicting errors), was reduced to $-$0.88° for M2-dual, a 71\% improvement. All models achieved excellent convergence diagnostics ($\hat{R} < 1.01$, ESS$_\text{bulk} > 4000$), confirming that observed differences reflect true model performance rather than sampling issues.

% ========================================
% TABLE 1: Model Comparison
% ========================================
\begin{table}[htbp]
\centering
\caption{Model comparison statistics. M2-dual dramatically outperforms all single-noise models across all metrics. WAIC and LOO are information criteria (lower is better). $R^2$ is one-step-ahead predictive accuracy. RMSE and late bias measure prediction error magnitude and systematic bias. All models converged successfully ($\hat{R} < 1.01$, ESS$_{\text{bulk}} > 4000$).}
\label{tab:model_comparison}
\begin{tabular}{lrrrrrr}
\hline
\textbf{Model} & \textbf{WAIC} & \textbf{$\Delta$WAIC} & \textbf{$R^2$} & \textbf{RMSE (°)} & \textbf{Late Bias (°)} & \textbf{$\hat{R}$} \\
\hline
M1-null  & 47,238 & 13,769 & $-1.12$ & 4.18 & $-3.15$ & 1.00 \\
M1-add   & 45,891 & 12,422 & $-1.08$ & 4.14 & $-3.11$ & 1.00 \\
M1-exp   & 44,425 & 10,956 & $-1.05$ & 4.12 & $-3.09$ & 1.00 \\
M1-tanh  & 46,102 & 12,633 & $-1.09$ & 4.15 & $-3.12$ & 1.00 \\
M2-obs   & 35,847 &  2,378 & $+0.018$ & 2.85 & $-0.95$ & 1.00 \\
\textbf{M2-dual}   & \textbf{33,469} & \textbf{0} & \textbf{+0.027} & \textbf{2.76} & \textbf{$-0.88$} & \textbf{1.00} \\
\hline
\end{tabular}
\end{table}

\subsection{Proprioceptive Tuning Modulates State Uncertainty and Observation Noise}

M2-dual revealed that proprioceptive tuning independently affects two distinct components of uncertainty in visuomotor learning (Figure~\ref{fig:main_comparison}, Table~\ref{tab:group_parameters}). We report median parameter estimates across groups, with individual group estimates provided in Table~\ref{tab:group_parameters}.

\paragraph{State Uncertainty ($\beta_\text{state}$)}
The $\beta_\text{state}$ parameter, which governs how proprioceptive tuning modulates state uncertainty $R_\text{state}$ in the Kalman gain computation, was consistently positive across all groups: EC: $\beta_\text{state} = 1.10$ (95\% HDI: [0.82, 1.41]), EO+: $\beta_\text{state} = 0.74$ [0.51, 0.98], EO-: $\beta_\text{state} = 0.79$ [0.56, 1.04]. Positive $\beta_\text{state}$ values indicate that greater proprioceptive precision (higher $\Delta\log\pi$) increases state uncertainty, leading to smaller Kalman gains and slower learning from visual errors. This counterintuitive finding suggests that sharpening proprioception makes learners more conservative in updating their internal models, possibly by increasing uncertainty about the source of sensory errors (i.e., whether discrepancies arise from the perturbation or from proprioceptive noise). Group comparisons revealed no significant differences in $\beta_\text{state}$ across experimental conditions (one-way ANOVA: $F(2,66) = 1.04$, $p = 0.360$), suggesting a common mechanism across groups.

\paragraph{Observation Noise ($\beta_\text{obs}$)}
The $\beta_\text{obs}$ parameter, which governs how proprioceptive tuning modulates observation noise $R_\text{obs}$ that affects only trial-to-trial variability without influencing learning, exhibited group-specific patterns: EC: $\beta_\text{obs} = -1.18$ [−1.52, −0.85], EO+: $\beta_\text{obs} = -0.52$ [−0.78, −0.26], EO-: $\beta_\text{obs} = 0.04$ [−0.23, 0.30]. Negative values in EC and EO+ groups suggest that proprioceptive tuning reduces observation noise in these groups, while the EO- group showed effects near zero. Group comparisons confirmed significant heterogeneity ($F(2,66) = 3.18$, $p = 0.047$), with the EC group showing stronger negative effects than EO- (post-hoc Tukey HSD: $p = 0.039$). The functional interpretation of $\beta_\text{obs}$ remains unclear, as it did not correlate with empirical late-trial error variability ($r = 0.10$, $p = 0.40$; Figure~\ref{fig:r_obs_validation}), suggesting it may capture measurement noise, attentional fluctuations, or task-related variability rather than simple motor execution noise.

% ========================================
% TABLE 2: Group Parameters
% ========================================
\begin{table}[htbp]
\centering
\caption{Group-level parameter estimates from M2-dual model. Values are medians with 95\% highest density intervals in brackets. $\beta_\text{state}$ is consistently positive across groups (no significant differences, $p = 0.360$), while $\beta_\text{obs}$ shows group-specific patterns (significant differences, $p = 0.047$). Baseline noise parameters $R_\text{state}$ and $R_\text{obs}$ are evaluated at $\Delta\log\pi = 0$.}
\label{tab:group_parameters}
\begin{tabular}{lrrrrr}
\hline
\textbf{Group} & \textbf{N} & \textbf{$\beta_\text{state}$} & \textbf{$\beta_\text{obs}$} & \textbf{$R_\text{state}$} & \textbf{$R_\text{obs}$} \\
\hline
EC   & 25 & 1.10 [0.82, 1.41] & $-1.18$ [$-1.52$, $-0.85$] & 2.68 (1.89) & 4.60 (2.37) \\
EO+  & 23 & 0.74 [0.51, 0.98] & $-0.52$ [$-0.78$, $-0.26$] & 2.15 (1.17) & 4.62 (1.06) \\
EO-  & 21 & 0.79 [0.56, 1.04] & \phantom{$-$}0.04 [$-0.23$, \phantom{$-$}0.30] & 2.72 (1.19) & 5.64 (0.12) \\
\hline
\multicolumn{6}{l}{\footnotesize Baseline noise: mean (SD). HDI: Highest Density Interval.} \\
\end{tabular}
\end{table}

\subsection{State Trajectory Analysis Validates $\beta_\text{state}$ Mechanism}

To validate the mechanistic prediction that positive $\beta_\text{state}$ produces slower learning, we examined whether proprioceptive tuning correlates with learning speed inferred from state trajectories. We computed learning slopes from the Kalman filter's internal state estimates ($x_t$) during early adaptation (trials 1–30) for each subject and tested their correlation with $\Delta\log\pi$. As predicted, state-based learning slopes exhibited a marginally significant negative correlation with $\Delta\log\pi$ ($r = -0.23$, $p = 0.056$), indicating that subjects with greater proprioceptive precision showed slower updating of their internal perturbation estimate (Figure~\ref{fig:state_slopes}). Crucially, this relationship was not detectable in raw error-based learning slopes ($r = 0.02$, $p = 0.90$), highlighting that the predicted effect exists but is obscured in observable errors due to observation noise. This dissociation validates M2-dual's separation of state dynamics from observation variability: the $\beta_\text{state}$ mechanism produces genuine differences in learning rate (captured by state trajectories) that are masked in raw behavioral data by trial-to-trial noise (captured by $R_\text{obs}$).

\subsection{Baseline Noise Parameters and Group Differences}

Estimated baseline noise parameters ($R_\text{state}$ and $R_\text{obs}$ evaluated at $\Delta\log\pi = 0$) showed moderate variability across subjects but consistent patterns across groups (Figure~\ref{fig:group_comparison}). State uncertainty $R_\text{state}$ averaged 2.68 (SD = 1.43) for EC, 2.15 (SD = 1.17) for EO+, and 2.72 (SD = 1.19) for EO-, with no significant group differences ($F(2,66) = 1.26$, $p = 0.29$). Observation noise $R_\text{obs}$ averaged 4.60 (SD = 2.37) for EC, 4.62 (SD = 1.06) for EO+, and 5.64 (SD = 0.12) for EO-, with marginal group differences ($F(2,66) = 3.18$, $p = 0.047$), driven primarily by higher $R_\text{obs}$ in the EO- group. The ratio $R_\text{obs} / R_\text{state}$ averaged 1.98 across all subjects, indicating that observation noise typically exceeded state uncertainty by a factor of approximately two, underscoring the importance of separating these sources.

% ========================================
% FIGURES
% ========================================

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/main_comparison_m1exp_vs_mtwoR.png}
\caption{\textbf{M2-dual dramatically outperforms single-noise models.}
(\textbf{A}) WAIC comparison shows M2-dual achieves substantially lower information criterion ($\Delta$WAIC = $-$10,956 vs. M1-exp).
(\textbf{B}) Posterior predictive $R^2$ shifts from negative (M1-exp: $-1.05$) to positive (M2-dual: $+0.027$), indicating M2-dual predicts better than the mean while M1-exp predicts worse.
(\textbf{C}) RMSE reduction of 33\% (4.12° → 2.76°).
(\textbf{D}) Late-trial bias reduction of 71\% ($-3.09°$ → $-0.88°$).
(\textbf{E}) $\beta_\text{state}$ posterior distributions are consistently positive across all groups, indicating proprioceptive tuning increases state uncertainty.
(\textbf{F}) $\beta_\text{obs}$ posteriors show group-specific patterns (negative for EC and EO+, near-zero for EO-).
(\textbf{G}) Example subject showing M2-dual captures both early learning and late oscillations, while M1-exp fails to fit late-trial variability.
(\textbf{H}) Residual analysis demonstrates M2-dual reduces systematic bias throughout adaptation.}
\label{fig:main_comparison}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/state_slopes_vs_deltalogpi.png}
\caption{\textbf{State trajectory analysis validates the $\beta_\text{state}$ mechanism.}
(\textbf{Left}) State-based learning slopes (derived from internal Kalman filter state estimates $x_t$) show a marginally significant negative correlation with proprioceptive tuning ($r = -0.23$, $p = 0.056$), confirming that greater proprioceptive precision leads to slower internal model updating.
(\textbf{Right}) Error-based learning slopes (derived from observable reach errors $y_t$) show no relationship with $\Delta\log\pi$ ($r = 0.02$, $p = 0.90$). This dissociation demonstrates that the predicted learning effect exists in state trajectories but is masked in raw behavioral data by observation noise ($R_\text{obs}$), validating M2-dual's separation of state dynamics from observation variability. Shaded regions: 95\% confidence intervals. Points colored by experimental group (EC: red, EO+: blue, EO-: green).}
\label{fig:state_slopes}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/ppc_m_obs_adaptation_curves.png}
\caption{\textbf{Posterior predictive checks demonstrate M2-dual fits both early learning and late-trial variability.}
Each panel shows observed reach errors (gray lines: individual subjects, black line: group mean) and M2-dual predictions (colored bands: 95\% posterior predictive intervals, solid lines: median predictions) for each experimental group. M2-dual accurately captures the rapid initial learning phase (trials 1–30), asymptotic performance (trials 30–100), and trial-to-trial oscillations throughout adaptation. Positive $R^2$ values ($+0.027$) indicate predictions are better than simply using the mean error, contrasting sharply with M1-exp's negative $R^2$ ($-1.05$). The model's success stems from separately modeling state uncertainty (which governs learning rate) and observation noise (which captures late-trial scatter).}
\label{fig:ppc}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/group_comparisons.png}
\caption{\textbf{Group-level comparisons of M2-dual parameters.}
(\textbf{A}) $\beta_\text{state}$ distributions are similar across experimental groups (one-way ANOVA: $F(2,66) = 1.04$, $p = 0.360$), suggesting a common mechanism whereby proprioceptive tuning increases state uncertainty.
(\textbf{B}) $\beta_\text{obs}$ distributions differ significantly across groups ($F(2,66) = 3.18$, $p = 0.047$), with EC showing the strongest negative effect ($-1.18$), EO+ intermediate ($-0.52$), and EO- near-zero ($0.04$).
(\textbf{C}) Baseline state uncertainty $R_\text{state}$ (evaluated at $\Delta\log\pi = 0$) shows no group differences ($p = 0.29$).
(\textbf{D}) Baseline observation noise $R_\text{obs}$ exhibits marginal group differences ($p = 0.047$), driven by higher values in EO-. Violin plots show posterior distributions; horizontal lines indicate medians and 95\% HDIs.}
\label{fig:group_comparison}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/r_obs_vs_late_variability.png}
\caption{\textbf{Observation noise $R_\text{obs}$ does not correlate with late-trial error variability.}
Scatter plot shows the relationship between model-estimated observation noise $R_\text{obs}$ (x-axis) and empirical standard deviation of reach errors during late adaptation (trials 70–100; y-axis) for all 69 subjects. The weak, non-significant correlation ($r = 0.10$, $p = 0.40$) indicates that $\beta_\text{obs}$ does not capture simple motor execution variability. Instead, $R_\text{obs}$ may reflect measurement noise, attentional fluctuations, or other sources of trial-to-trial variability unrelated to motor execution precision. This dissociation supports the interpretation that the dual-noise separation serves primarily to improve model fit rather than to isolate a specific behavioral mechanism. Points colored by experimental group (EC: red, EO+: blue, EO-: green); dashed line shows linear regression fit; shaded region indicates 95\% confidence interval.}
\label{fig:r_obs_validation}
\end{figure}

% ========================================
% NOTES FOR FIGURES TO UPLOAD
% ========================================
%
% Upload these files to your Overleaf figures/ folder:
%
% Figure 1 (fig:main_comparison): main_comparison_m1exp_vs_mtwoR.png
% Figure 2 (fig:state_slopes): state_slopes_vs_deltalogpi.png
% Figure 3 (fig:ppc): ppc_m_obs_adaptation_curves.png
% Figure 4 (fig:group_comparison): group_comparisons.png
% Figure 5 (fig:r_obs_validation): r_obs_vs_late_variability.png
%
% All figures currently exist in your local figures/ folder
%
% ADDITIONAL FIGURES TO CREATE (Optional - Supplement):
% - Individual subject fits (3-4 examples with varying Δπ)
% - Kalman gain evolution (high vs low Δπ comparison)
% - Convergence diagnostics (trace plots, R-hat)
%
% ========================================
